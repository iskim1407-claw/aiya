'use client'

import { useState, useEffect, useRef, useCallback } from 'react'

type State = 'init' | 'recording' | 'processing' | 'speaking'

export default function ChildPage() {
  const [state, setState] = useState<State>('init')
  const [response, setResponse] = useState('')
  const [lastHeard, setLastHeard] = useState('')
  const [history, setHistory] = useState<{role: string, content: string}[]>([])
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const streamRef = useRef<MediaStream | null>(null)
  const stateRef = useRef<State>('init')
  const recordingTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const autoRestartRef = useRef(true)

  const updateState = useCallback((newState: State) => {
    setState(newState)
    stateRef.current = newState
  }, [])

  // TTS (5ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ stuck ë°©ì§€)
  const speak = useCallback((text: string, audioData?: string, onEnd?: () => void) => {
    if (typeof window === 'undefined') { onEnd?.(); return }
    if (audioRef.current) { audioRef.current.pause(); audioRef.current = null }
    window.speechSynthesis?.cancel()
    
    let finished = false
    const finish = () => {
      if (finished) return
      finished = true
      onEnd?.()
    }
    
    // 5ì´ˆ í›„ ê°•ì œ ì§„í–‰ (stuck ë°©ì§€)
    const timeout = setTimeout(() => {
      console.log('[TTS íƒ€ì„ì•„ì›ƒ - ê°•ì œ ì§„í–‰]')
      finish()
    }, 5000)
    
    const done = () => {
      clearTimeout(timeout)
      finish()
    }
    
    console.log('[TTS]', text, audioData ? '(OpenAI)' : '(ë¸Œë¼ìš°ì €)')
    
    if (audioData) {
      const audio = new Audio(audioData)
      audioRef.current = audio
      audio.onended = () => { console.log('[Audio ì™„ë£Œ]'); done() }
      audio.onerror = (e) => { console.log('[Audio ì—ëŸ¬]', e); done() }
      audio.play()
        .then(() => console.log('[Audio ì¬ìƒ ì‹œì‘]'))
        .catch((e) => { console.log('[Audio ì¬ìƒ ì‹¤íŒ¨]', e); done() })
      return
    }
    
    // ë¸Œë¼ìš°ì € TTS fallback
    const utterance = new SpeechSynthesisUtterance(text)
    utterance.lang = 'ko-KR'
    utterance.rate = 0.9
    utterance.pitch = 1.2
    utterance.onend = () => done()
    utterance.onerror = () => done()
    window.speechSynthesis.speak(utterance)
  }, [])

  // ì‘ë³„ ì¸ì‚¬ ì²´í¬
  const isGoodbye = (text: string): boolean => {
    const goodbyes = ['ì˜ê°€', 'ì˜ ê°€', 'ë°”ì´', 'ë°”ì´ë°”ì´', 'ë', 'ê·¸ë§Œ', 'ë‹¤ìŒì—', 'ë‚˜ì¤‘ì—', 'ì•ˆë…•']
    return goodbyes.some(g => text.includes(g))
  }

  // Whisperë¡œ ì˜¤ë””ì˜¤ ì²˜ë¦¬
  const processAudio = useCallback(async (audioBlob: Blob) => {
    updateState('processing')
    
    try {
      const formData = new FormData()
      formData.append('audio', audioBlob, 'audio.webm')
      formData.append('history', JSON.stringify(history))
      
      const res = await fetch('/api/talk-whisper', { method: 'POST', body: formData })
      const data = await res.json()
      
      if (!data.ok || !data.transcript) {
        console.log('[ì¸ì‹ ì•ˆë¨]')
        if (autoRestartRef.current) {
          updateState('recording')
          startRecording()
        }
        return
      }

      const { transcript, message, audio } = data
      setLastHeard(transcript)
      console.log('[ì¸ì‹ë¨]', transcript)
      
      // íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
      setHistory(prev => [...prev, { role: 'user', content: transcript }, { role: 'assistant', content: message }].slice(-10))
      
      // ì‘ë³„ ì²´í¬
      if (isGoodbye(transcript)) {
        setResponse('ì‘! ë˜ ë†€ì! ğŸ‘‹')
        autoRestartRef.current = false
        speak('ì‘! ë˜ ë†€ì! ì•ˆë…•!', audio, () => {
          updateState('init')
          setResponse('')
          setLastHeard('')
          setHistory([])
        })
        return
      }
      
      setResponse(message)
      updateState('speaking')
      
      speak(message, audio, () => {
        if (autoRestartRef.current) {
          updateState('recording')
          startRecording()
        }
      })
      
    } catch (error) {
      console.error('[ì²˜ë¦¬ ì˜¤ë¥˜]', error)
      if (autoRestartRef.current) {
        updateState('recording')
        startRecording()
      }
    }
  }, [history, speak])

  // ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    try {
      if (!streamRef.current) {
        streamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true })
      }
      
      const mediaRecorder = new MediaRecorder(streamRef.current, { mimeType: 'audio/webm' })
      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []
      
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) audioChunksRef.current.push(e.data)
      }
      
      mediaRecorder.onstop = () => {
        if (audioChunksRef.current.length > 0) {
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
          processAudio(audioBlob)
        }
      }
      
      mediaRecorder.start()
      updateState('recording')
      console.log('[ë…¹ìŒ ì‹œì‘]')
      
      // 3ì´ˆ ë…¹ìŒ
      recordingTimeoutRef.current = setTimeout(() => {
        if (mediaRecorderRef.current?.state === 'recording') {
          mediaRecorderRef.current.stop()
          console.log('[ë…¹ìŒ ì™„ë£Œ]')
        }
      }, 3000)
      
    } catch (error) {
      console.error('[ë…¹ìŒ ì˜¤ë¥˜]', error)
    }
  }, [processAudio])

  // ì‹œì‘ ë²„íŠ¼
  const handleStart = useCallback(async () => {
    console.log('[ì‹œì‘ ë²„íŠ¼ í´ë¦­]')
    setResponse('ì‹œì‘ ì¤‘...')
    
    try {
      // ì˜¤ë””ì˜¤ unlock
      const silentAudio = new Audio('data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU4Ljc2LjEwMAAAAAAAAAAAAAAA//tQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAABhgC7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7//////////////////////////////////////////////////////////////////8AAAAATGF2YzU4LjEzAAAAAAAAAAAAAAAAJAAAAAAAAAAAAYYoRwmHAAAAAAD/+9DEAAAIAANIAAAAgAAA0gAAABBMTEhJSWBgYGBgVFRVRcXFxMTExcXFhYWFkZGRpaWloKChsbGxqamprq6utra2u7u7wcHBxsbGy8vL0dHR1tbW3Nzc4eHh5ubm7Ozs8fHx9vb2+/v7//8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//tQxAADwAADSAAAAAIAA0gAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')
      try { await silentAudio.play() } catch (e) { console.log('[Audio unlock ì‹¤íŒ¨]', e) }
      
      window.speechSynthesis?.cancel()
      
      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      console.log('[ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­]')
      streamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true })
      console.log('[ë§ˆì´í¬ ê¶Œí•œ ì„±ê³µ]')
      
      autoRestartRef.current = true
      setResponse('')
      
      // ì¸ì‚¬í•˜ê³  ë…¹ìŒ ì‹œì‘
      speak('ì‘! ë­ì•¼?', undefined, () => {
        startRecording()
      })
    } catch (e: any) {
      console.error('[ì‹œì‘ ì—ëŸ¬]', e)
      setResponse('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•´ìš”! ğŸ¤')
    }
  }, [speak, startRecording])

  // ì •ì§€ ë²„íŠ¼
  const handleStop = useCallback(() => {
    autoRestartRef.current = false
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop()
    }
    if (recordingTimeoutRef.current) clearTimeout(recordingTimeoutRef.current)
    updateState('init')
    setResponse('')
    setLastHeard('')
    setHistory([])
    speak('ë˜ ë¶ˆëŸ¬ì¤˜!')
  }, [speak])

  // ì •ë¦¬
  useEffect(() => {
    return () => {
      if (recordingTimeoutRef.current) clearTimeout(recordingTimeoutRef.current)
      if (streamRef.current) streamRef.current.getTracks().forEach(t => t.stop())
    }
  }, [])

  // ì‹œì‘ í™”ë©´
  if (state === 'init') {
    return (
      <main className="flex flex-col items-center justify-center min-h-screen p-4 bg-gradient-to-b from-pink-200 via-purple-100 to-blue-200">
        <div className="text-9xl mb-8 animate-bounce">ğŸ¤—</div>
        <h1 className="text-5xl font-bold text-purple-800 mb-4">ì•„ì´ì•¼!</h1>
        <p className="text-xl text-purple-600 mb-8">AI ì¹œêµ¬ì™€ ëŒ€í™”í•´ìš”</p>
        <button
          onClick={handleStart}
          className="px-12 py-6 bg-gradient-to-r from-pink-500 to-purple-500 text-white text-3xl font-bold rounded-full shadow-2xl active:scale-95 transition-transform"
        >
          ğŸ¤ ëŒ€í™”í•˜ê¸°
        </button>
      </main>
    )
  }

  return (
    <main className="flex flex-col items-center justify-center min-h-screen p-4 bg-gradient-to-b from-pink-200 via-purple-100 to-blue-200">
      <div className="text-center mb-6">
        <div className={`text-8xl mb-4 ${state === 'recording' ? 'animate-bounce' : state === 'processing' ? 'animate-pulse' : ''}`}>
          {state === 'recording' ? 'ğŸ‘‚' : state === 'processing' ? 'ğŸ¤”' : 'ğŸ—£ï¸'}
        </div>
        <h1 className="text-5xl font-bold text-purple-800 mb-3">ì•„ì´ì•¼!</h1>
        <p className="text-lg text-purple-600">
          {state === 'recording' ? 'ë“£ê³  ìˆì–´ìš”...' : state === 'processing' ? 'ìƒê° ì¤‘...' : 'ë§í•˜ëŠ” ì¤‘...'}
        </p>
      </div>

      {lastHeard && (
        <div className="mb-4 px-5 py-2 bg-white/60 rounded-full text-gray-700 text-lg">
          ğŸ§ "{lastHeard}"
        </div>
      )}

      {response && (
        <div className="mt-2 p-6 bg-white rounded-3xl shadow-xl max-w-sm text-center">
          <p className="text-2xl text-gray-800 leading-relaxed font-bold">{response}</p>
        </div>
      )}

      {state === 'processing' && (
        <div className="mt-6">
          <div className="animate-spin rounded-full h-14 w-14 border-4 border-purple-300 border-t-purple-600"></div>
        </div>
      )}

      <button
        onClick={handleStop}
        className="fixed bottom-8 px-8 py-4 bg-red-400 text-white text-xl font-bold rounded-full shadow-lg active:scale-95 transition-transform"
      >
        ğŸ‘‹ ëë‚´ê¸°
      </button>
    </main>
  )
}
